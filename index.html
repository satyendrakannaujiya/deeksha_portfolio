<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="./bootstrap.min.css">
    <link rel="icon" type="image/x-icon" href="./assets/favicon.png">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/js/all.min.js"></script>
    
    <title>Deeksha Varshney</title>
    <style>
        .ex_img{
            height:100px;
            width:100px;
        }

        .footer1{
   position: fixed;
   left: 0;
   bottom: 0;
   width: 100%;
   background-color: rgb(247, 247, 247);;
   color: black;
   text-align: center;
   padding:10px;
}
       
    </style>
</head>
<body>
   
   <!-------------------HEADINGS-------------------------------->
     <nav class="navbar navbar-expand-lg navbar-light bg-light navbar-fixed-top" style="width: 100%;padding-left: 5%;padding-right: 5%;">
        <a class="navbar-brand" href="#"><strong>Deeksha Varshney</strong></a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>

      
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <ul class="navbar-nav mr-auto" style="float:right;">
                        <li class="nav-item">
                        <a class="nav-link" href="#" style="color:black !important;">Home&nbsp;</a>
                        </li>
                        <li class="nav-item">
                          <a class="nav-link" href="#experience" style="color:black !important;">Experience&nbsp;</a>
                        </li>
                        <li class="nav-item">
                        <a class="nav-link" href="#publication" style="color:black !important;">Publication&nbsp;</a>
                        </li>
                        <li class="nav-item">
                        <a class="nav-link" href="#blogs" style="color:black !important;">Blogs&nbsp;</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#news" style="color:black !important;">News&nbsp;</a>
                        </li>
            </ul>
        </div>
      </nav> 

    

      <div class="container" style="margin-top:100px;">
    
        <div class="row" itemprop="author" itemscope="" itemtype="http://schema.org/Person" style="user-select: auto;">
            <div class="col-xs-12 col-md-4" style="user-select: auto;">
              <div id="profile" style="user-select: auto;">
                <div style="display:flex;flex-direction:row;justify-content:space-around;">
                    <img src="./assets/diksha.jpeg" alt="" style="border-radius: 50%;height:250px;width:250px;">
                </div>
                <div style="text-align: center;">
                  <h4>Deeksha Varshney</h4>
                </div>
                <div style="text-align: center;">
                     <a href="./assets/resume.pdf" target="_blank"><i class="fa-solid fa-file-pdf"></i></a>
                     <a href="https://github.com/satyendrakannaujiya" target="_blank"><i class="fa-brands fa-github"></i></a>
                     <a href="https://github.com/satyendrakannaujiya" target="_blank"><i class="fa-brands fa-google-plus"></i></a>
                     <a href="https://github.com/satyendrakannaujiya" target="_blank"> <i class="fa-brands fa-facebook"></i></a>
                      <a href="https://github.com/satyendrakannaujiya" target="_blank"><i class="fa-brands fa-twitter"></i></a>
                      <a href="https://github.com/satyendrakannaujiya" target="_blank"> <i class="fa-brands fa-linkedin"></i></a>
                </div>
              </div>
            </div>

            <!-----------------------------------------BIO------------------------------->
      
            <div class="visible-sm visible-xs" style="user-select: auto;"></div>
            <div class="col-xs-12 col-md-8" itemprop="description" style="user-select: auto;">
                    <h1 id="biography" style="user-select: auto;">Bio</h1>
                    <p style="user-select: auto;"> I'm currently working at <a href="https://salesken.ai/" style="user-select: auto;">Salesken</a> as Machine Learning Software Engineer. My interest lies at the intersection of computer vision, natural language processing and reinforcement learning.</p>
                    <p style="user-select: auto;"> Prior to this, I was a visiting research scholar at AI-NLP-ML Lab (IIT Patna) advised by <a href="https://www.iitp.ac.in/~asif/" style="user-select: auto;">Prof. Asif Ekbal</a> and <a href="https://www.cse.iitb.ac.in/~pb/" style="user-select: auto;">Prof. Pushpak Bhattacharyya</a>. I graduated with a Bachelor of Technology (B.Tech) from IIIT Bhubaneswar major in Electrical and Electronics Engineering with a minor in Computer Science.</p>
              </div>
      
            </div>
    
    </div>

    <br>
    <br>
    
  <!-----------------------------------------EXPERIENCE------------------------------->
  <h3 style="text-align:center;">Experience</h3>
  <hr>
    <div id="experience" style="width:80%;margin:auto;border:0px solid black;display:flex;flex-direction:row;justify-content:center;">
         
            <div class="experience_div" style="display:inline-block;">
                    <div style="display:flex;flex-direction:column;align-items:center;padding:5px;">
                            <div>
                              <img src="./assets/isc.png" alt="" style="border-radius: 50%;" class="ex_img">
                            </div>
                              <br />
                              <div style="text-align: center;">
                                <strong>Bsc(Hons) Computer Science</strong>
                                <br />
                                ISC BHU
                                <br />
                                July 2013 - May 2016
                              </div>
                    </div>
              </div>
          <div class="experience_div" style="display:inline-block;">
                <div style="display:flex;flex-direction:column;align-items:center;padding:5px;">
                        <div>
                          <img src="./assets/isc.png" alt="" style="border-radius: 50%;" class="ex_img">
                        </div>
                          <br />
                          <div style="text-align: center;">
                            <strong>Msc Computer Science</strong>
                            <br />
                            ISC BHU
                            <br />
                            July 2016 - May 2018
                          </div>
                </div>
          </div>
          <div class="experience_div" style="display:inline-block;">
            <div style="display:flex;flex-direction:column;align-items:center;padding:5px;">
                    <div>
                      <img src="./assets/iit_patna.jpeg" alt="" style="border-radius: 50%;" class="ex_img">
                    </div>
                      <br />
                      <div style="text-align: center;">
                        <strong>PHD</strong>
                        <br />
                        IIT Patna
                        <br />
                        July 2018 - 
                      </div>
            </div>
       </div>

    </div>


    <!-----------------------------------------PUBLICATIONS------------------------------->
 <br />
 <br />
 <h3 style="text-align:center;">Publication</h3>
 <hr>
    <div id="publication" style="width:80%;margin:auto;">

      <!-----------------------------------------PUBLICATIONS 1 details------------------------------->
        <div class="publication_div" style="display:flex;flex-direction:row;min-height:280px;flex-wrap:wrap;align-items:center;justify-content:flex-start;">
             <div class="pub_img" style="width:30%;">
              <img src="./assets/pub.png" alt="" style="border-radius: 50%;height:250px;width:250px;" class="ex_img">
             </div>
            
             <div class="pub_details">
                    <div>
                     <span><i class="fa fa-file-text"></i></span>&nbsp;&nbsp;<strong>More to Diverse: Generating Diversified Responses in Visual Dialog</strong>
                    </div>
                    <div>
                      <a href="#">Arnav Shindey</a>,<a href="#">Arnav Shindey</a>,<a href="#">Arnav Shindey</a>
                    </div>
                    <div style="color:#090">
                      Proceedings at 31st International Joint Conference on Neural Networks 2020, Glasgow, United Kingdom (Oral)
                    </div>
                    <br>
                    <div>
                          <a class="btn btn-primary btn-outline btn-xs" href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0241271" style="user-select: auto;">
                            PDF
                          </a>
                          <a class="btn btn-primary btn-outline btn-xs" onclick="
                          var div = document.getElementById('steal_bib');
                          if (div.style.display !== 'none') {
                          div.style.display = 'none';
                          }
                          else {
                          div.style.display = 'block';
                          }
                          " style="user-select: auto;">
                          BibTeX
                          </a>
      
                  </div>
                  <div class="pub_code" id="text" style="overflow-wrap: break-word;">
                    <br>
                    <pre id="steal_bib" ,="" style="display: none;width:100%;overflow-wrap: break-word;width:700px;">@article{10.1371/journal.pone.0241271,
                      author = {Firdaus, Mauajama AND Pratap Shandeelya, Arunav AND Ekbal, Asif},
                      journal = {PLOS ONE},
                      publisher = {Public Library of Science},
                      title = {More to diverse: Generating diversified responses in a task oriented multimodal dialog system},
                      year = {2020},
                      month = {11},
                      volume = {15},
                      url = {https://doi.org/10.1371/journal.pone.0241271},
                      pages = {1-26},
                      abstract = {Multimodal dialogue system, due to its many-fold applications, has gained much attention to the researchers and developers in recent times. With the release of large-scale multimodal dialog dataset Saha et al. 2018 on the fashion domain, it has been possible to investigate the dialogue systems having both textual and visual modalities. Response generation is an essential aspect of every dialogue system, and making the responses diverse is an important problem. For any goal-oriented conversational agent, the system’s responses must be informative, diverse and polite, that may lead to better user experiences. In this paper, we propose an end-to-end neural framework for generating varied responses in a multimodal dialogue setup capturing information from both the text and image. Multimodal encoder with co-attention between the text and image is used for focusing on the different modalities to obtain better contextual information. For effective information sharing across the modalities, we combine the information of text and images using the BLOCK fusion technique that helps in learning an improved multimodal representation. We employ stochastic beam search with Gumble Top K-tricks to achieve diversified responses while preserving the content and politeness in the responses. Experimental results show that our proposed approach performs significantly better compared to the existing and baseline methods in terms of distinct metrics, and thereby generates more diverse responses that are informative, interesting and polite without any loss of information. Empirical evaluation also reveals that images, while used along with the text, improve the efficiency of the model in generating diversified responses.},
                      number = {11},
                      doi = {10.1371/journal.pone.0241271}
                      }
                      </pre>
                  </div>
            </div>

    </div>

     <!-----------------------------------------PUBLICATIONS 2 details------------------------------->

     <div class="publication_div" style="display:flex;flex-direction:row;min-height:280px;flex-wrap:wrap;align-items:center;justify-content:flex-start;">
      <div class="pub_img" style="width:30%;">
       <img src="./assets/pub2.png" alt="" style="border-radius: 50%;height:250px;width:250px;" class="ex_img">
      </div>
     
      <div class="pub_details">
             <div>
              <span><i class="fa fa-file-text"></i></span>&nbsp;&nbsp;<strong>More to Diverse: Generating Diversified Responses in Visual Dialog</strong>
             </div>
             <div>
               <a href="#">Arnav Shindey</a>,<a href="#">Arnav Shindey</a>,<a href="#">Arnav Shindey</a>
             </div>
             <div style="color:#090">
               Proceedings at 31st International Joint Conference on Neural Networks 2020, Glasgow, United Kingdom (Oral)
             </div>
             <br>
             <div>
                   <a class="btn btn-primary btn-outline btn-xs" href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0241271" style="user-select: auto;">
                     PDF
                   </a>
                   <a class="btn btn-primary btn-outline btn-xs" onclick="
                   var div = document.getElementById('steal_bib2');
                   if (div.style.display !== 'none') {
                   div.style.display = 'none';
                   }
                   else {
                   div.style.display = 'block';
                   }
                   " style="user-select: auto;">
                   BibTeX
                   </a>

           </div>
           <div class="pub_code" id="text" style="overflow-wrap: break-word;">
             <br>
             <pre id="steal_bib2" ,="" style="display: none;width:100%;overflow-wrap: break-word;width:700px;">@article{10.1371/journal.pone.0241271,
               author = {Firdaus, Mauajama AND Pratap Shandeelya, Arunav AND Ekbal, Asif},
               journal = {PLOS ONE},
               publisher = {Public Library of Science},
               title = {More to diverse: Generating diversified responses in a task oriented multimodal dialog system},
               year = {2020},
               month = {11},
               volume = {15},
               url = {https://doi.org/10.1371/journal.pone.0241271},
               pages = {1-26},
               abstract = {Multimodal dialogue system, due to its many-fold applications, has gained much attention to the researchers and developers in recent times. With the release of large-scale multimodal dialog dataset Saha et al. 2018 on the fashion domain, it has been possible to investigate the dialogue systems having both textual and visual modalities. Response generation is an essential aspect of every dialogue system, and making the responses diverse is an important problem. For any goal-oriented conversational agent, the system’s responses must be informative, diverse and polite, that may lead to better user experiences. In this paper, we propose an end-to-end neural framework for generating varied responses in a multimodal dialogue setup capturing information from both the text and image. Multimodal encoder with co-attention between the text and image is used for focusing on the different modalities to obtain better contextual information. For effective information sharing across the modalities, we combine the information of text and images using the BLOCK fusion technique that helps in learning an improved multimodal representation. We employ stochastic beam search with Gumble Top K-tricks to achieve diversified responses while preserving the content and politeness in the responses. Experimental results show that our proposed approach performs significantly better compared to the existing and baseline methods in terms of distinct metrics, and thereby generates more diverse responses that are informative, interesting and polite without any loss of information. Empirical evaluation also reveals that images, while used along with the text, improve the efficiency of the model in generating diversified responses.},
               number = {11},
               doi = {10.1371/journal.pone.0241271}
               }
               </pre>
           </div>
     </div>

</div>


<!--------------------------- BLOGS------------------------------>

<br />
<br />
<h3 style="text-align:center;">Blogs</h3>
<hr>

<section id="blogs" style="background-color:rgb(247, 247, 247);">

  <!------------BLOG 1 details ---------------------->
      <div class="container" >
                  <h3 id="sbert" style="user-select: auto;"><a href="https://chatbotslife.com/fast-semantic-search-using-sentence-bert-4123b27022ed" style="user-select: auto;">Fast Semantic Search Using Sentence BERT</a></h3>

                  <p style="user-select: auto;"> Are BERT and RoBERTa have computationally powerful for Semantically Textual Similarity?. Expolore and Check out with implementation with the Blogspot</p>
      </div>

        <hr>
       <!------------BLOG 2 details ---------------------->
       <div class="container" style="user-select: auto;">
        <h3 id="sbert" style="user-select: auto;"><a href="https://chatbotslife.com/fast-semantic-search-using-sentence-bert-4123b27022ed" style="user-select: auto;">Fast Semantic Search Using Sentence BERT</a></h3>

        <p style="user-select: auto;"> Are BERT and RoBERTa have computationally powerful for Semantically Textual Similarity?. Expolore and Check out with implementation with the Blogspot</p>
      </div>
   
</section>



<!--------------------------- Latest Update ------------------------------>

<br />
<br />
<h3 style="text-align:center;">Recent Activity</h3>
<hr>

<div id="news" style="border:0px solid black;">                  
        <p><strong>[Sep 2021]&nbsp;</strong>Attended Stanford Graph Learning <a href="https://snap.stanford.edu/graphlearning-workshop/index.html">Workshop</a>  </p>
        <p><strong>[Sep 2022]&nbsp;</strong>Joined as Machine Learning Engineer @ Salesken AI <a href="https://snap.stanford.edu/graphlearning-workshop/index.html">Workshop</a>  </p>
        <p><strong>[Sep 2021]&nbsp;</strong> Journal "More to Diverse: Generating Diversified Responses in Visual Dialog System" available <a href="https://snap.stanford.edu/graphlearning-workshop/index.html" >Workshop</a>  </p>
        <p><strong>[Sep 2021]&nbsp;</strong>Attended Stanford Graph Learning <a href="https://snap.stanford.edu/graphlearning-workshop/index.html">Workshop</a>  </p>
        <p><strong>[Sep 2021]&nbsp;</strong>Attended Stanford Graph Learning <a href="https://snap.stanford.edu/graphlearning-workshop/index.html">Workshop</a>  </p>
        <p><strong>[Sep 2022]&nbsp;</strong>Joined as Machine Learning Engineer @ Salesken AI <a href="https://snap.stanford.edu/graphlearning-workshop/index.html">Workshop</a>  </p>
        <p><strong>[Sep 2021]&nbsp;</strong> Journal "More to Diverse: Generating Diversified Responses in Visual Dialog System" available <a href="https://snap.stanford.edu/graphlearning-workshop/index.html" >Workshop</a>  </p>
        <p><strong>[Sep 2021]&nbsp;</strong>Attended Stanford Graph Learning <a href="https://snap.stanford.edu/graphlearning-workshop/index.html">Workshop</a>  </p>
        <p><strong>[Sep 2021]&nbsp;</strong>Attended Stanford Graph Learning <a href="https://snap.stanford.edu/graphlearning-workshop/index.html">Workshop</a>  </p>
        <p><strong>[Sep 2022]&nbsp;</strong>Joined as Machine Learning Engineer @ Salesken AI <a href="https://snap.stanford.edu/graphlearning-workshop/index.html">Workshop</a>  </p>
        <p><strong>[Sep 2021]&nbsp;</strong> Journal "More to Diverse: Generating Diversified Responses in Visual Dialog System" available <a href="https://snap.stanford.edu/graphlearning-workshop/index.html" >Workshop</a>  </p>
        <p><strong>[Sep 2021]&nbsp;</strong>Attended Stanford Graph Learning <a href="https://snap.stanford.edu/graphlearning-workshop/index.html">Workshop</a>  </p>
        <p><strong>[Sep 2021]&nbsp;</strong>Attended Stanford Graph Learning <a href="https://snap.stanford.edu/graphlearning-workshop/index.html">Workshop</a>  </p>
        <p><strong>[Sep 2022]&nbsp;</strong>Joined as Machine Learning Engineer @ Salesken AI <a href="https://snap.stanford.edu/graphlearning-workshop/index.html">Workshop</a>  </p>
        <p><strong>[Sep 2021]&nbsp;</strong> Journal "More to Diverse: Generating Diversified Responses in Visual Dialog System" available <a href="https://snap.stanford.edu/graphlearning-workshop/index.html" >Workshop</a>  </p>
        <p><strong>[Sep 2021]&nbsp;</strong>Attended Stanford Graph Learning <a href="https://snap.stanford.edu/graphlearning-workshop/index.html">Workshop</a>  </p>
        
</div>




<div style="height:100px;">
</div>


<!---------------FOOTER------------------->
<!-- <br />
<br />
<hr> -->


<div class="footer1">
  <p> © 2022 Deeksha Varshney </p>
</div>


</body>
</html>


